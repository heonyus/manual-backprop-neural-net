# Manual Backprop Neural Net: Deconstructing Deep Learning

ğŸŒ [English](README.md) | **í•œêµ­ì–´**

![image](https://i.imgur.com/qrYfsnh.png)


## 1. Project Overview
> **"What I cannot create, I do not understand."** - Richard Feynman

ì´ í”„ë¡œì íŠ¸ëŠ” **PyTorchì˜ `autograd` ì—”ì§„ ì—†ì´**, ì˜¤ì§ `NumPy`ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ ê³¼ì •(Forward, Backward, Optimizer)ì„ ë°‘ë°”ë‹¥ë¶€í„° êµ¬í˜„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
ë¸”ë™ë°•ìŠ¤ë¡œ ì—¬ê²¨ì§€ë˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë¥¼ ì—­ê³µí•™(Reverse Engineering)í•˜ì—¬, ê³„ì‚° ê·¸ë˜í”„(Computational Graph)ì™€ ì—­ì „íŒŒ(Backpropagation)ì˜ ìˆ˜í•™ì  ë³¸ì§ˆì„ ì´í•´í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

## 2. Key Features
* **Pure NumPy Implementation:** `torch.autograd` ì˜ì¡´ì„± 0%.
* **Modular Design:** `Layer` ê¸°ë°˜ì˜ ê°ì²´ ì§€í–¥ ì„¤ê³„.
* **Mathematical Rigor:** ì—°ì‡„ ë²•ì¹™(Chain Rule)ì— ê¸°ë°˜í•œ ì •í™•í•œ ê¸°ìš¸ê¸° ê³„ì‚°.

## 3. Mathematical Foundations
ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì—°ì‡„ ë²•ì¹™(Chain Rule)ì„ í†µí•´ êµ­ì†Œì  ë¯¸ë¶„(Local Gradient)ì„ ìƒë¥˜(Upstream)ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}
$$

íŠ¹íˆ, `Softmax-with-Loss` ê³„ì¸µì˜ ì—­ì „íŒŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìš°ì•„í•˜ê²Œ ìœ ë„ë¨ì„ ì½”ë“œ ë ˆë²¨ì—ì„œ ì¦ëª…í•©ë‹ˆë‹¤.

$$
\frac{\partial L}{\partial z_k} = y_k - t_k
$$

(ì—¬ê¸°ì„œ $y_k$ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥, $t_k$ëŠ” ì •ë‹µ ë ˆì´ë¸”ì…ë‹ˆë‹¤.)

## 4. Verification
êµ¬í˜„ì˜ ì •í™•ì„±ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì—„ê²©í•˜ê²Œ ê²€ì¦ë©ë‹ˆë‹¤.
1. **Gradient Checking:** ìˆ˜ì¹˜ ë¯¸ë¶„(Numerical Differentiation)ê³¼ì˜ ë¹„êµ.
2. **Cross-Validation with PyTorch:** PyTorchì˜ ìë™ ë¯¸ë¶„ ê²°ê³¼ì™€ $10^{-5}$ ì´í•˜ì˜ ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì¼ì¹˜ í™•ì¸.
